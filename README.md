# DL_Homework_Assignment_4

# Deep-Learning
## Natural Language Processing and Transformers ‚Äì Homework Assignment 4

### Student Information
* **Name:** Mounika Bolla
* **Student ID:** 700757766
* **Course:** CS 5720 Neural Networks and Deep Learning

## üîç NLP & Transformers Implementation ‚Äì Homework Assignment 4

This repository contains a Google Colab notebook titled `Homework_Assignment_4_700757766.ipynb`, which demonstrates the implementation of various **Natural Language Processing techniques** and **Transformer-based models**.

## üìå Overview

This assignment explores fundamental NLP preprocessing techniques, named entity recognition, attention mechanisms, and the application of pre-trained transformer models for sentiment analysis. All implementations use Python with libraries like NLTK, spaCy, NumPy, and Hugging Face Transformers.

## üìì Contents of the Notebook

The notebook includes the following major sections:

* üî§ **NLP Preprocessing Pipeline**
  * Tokenization
  * Stopword removal
  * Stemming

* üè∑Ô∏è **Named Entity Recognition with spaCy**
  * Extracting named entities from text
  * Entity labeling and position tracking

* üî¢ **Scaled Dot-Product Attention**
  * Implementation of the core attention mechanism
  * Mathematical computation of attention weights

* üòä **Sentiment Analysis with Transformers**
  * Using pre-trained models from Hugging Face
  * Sentiment classification with confidence scores

* üìù **Short Answer Questions**
  * Theoretical explanations of NLP concepts
  * Comparisons between different techniques and models

## üíª Environment

This notebook was developed in **Google Colab**, which includes:
* Python 3.x
* NLTK
* spaCy
* NumPy
* Hugging Face Transformers

To run this notebook:
1. Open Google Colab
2. Upload the `.ipynb` file
3. Run all cells to reproduce the workflow

## Submission Instructions

1. The source code has been uploaded to this GitHub repository.
2. A video demonstration (2-3 minutes) explaining the code and results has been recorded and submitted.
3. This README file provides an overview and explanation of the assignment.

## Video Demonstration

[Link to video demonstration]

## Repository Link

[https://github.com/Mounika-Bolla/Deep-Learning]

## üß∞ Required Libraries

```
pip install nltk spacy numpy transformers torch
python -m spacy download en_core_web_sm
```

## Key Concepts Covered

1. **Natural Language Processing Basics**
   * Tokenization strategies
   * Stop word removal importance
   * Word stemming vs. lemmatization

2. **Named Entity Recognition**
   * Entity extraction techniques
   * Classification of named entities

3. **Attention Mechanisms**
   * Mathematical foundations of attention
   * Implementation details of scaled dot-product attention

4. **Transformer Models**
   * Pre-trained model usage
   * Architectural differences between BERT and GPT
   * Benefits of transfer learning in NLP
